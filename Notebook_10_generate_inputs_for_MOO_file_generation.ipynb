{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6e64bb95",
   "metadata": {},
   "source": [
    "# Generation of data tables with data required for creating input files for multiobjective dam placement study in Myanmar\n",
    "### T. Janus\n",
    "### Mui Ne, 28/12/2023"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b70844c",
   "metadata": {},
   "source": [
    "**NOTE:** Use IFC names throughout!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "449ca9aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "from typing import List, Dict\n",
    "import pathlib\n",
    "from dataclasses import dataclass\n",
    "import numpy as np\n",
    "import geopandas as gpd\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e917f53b",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class PywrIFCMapper:\n",
    "    \"\"\" \"\"\"\n",
    "    name_map: pd.DataFrame\n",
    "        \n",
    "    @classmethod\n",
    "    def from_file(cls, file_path: pathlib.Path) -> PywrIFCMapper:\n",
    "        \"\"\" \"\"\"\n",
    "        return cls(name_map=pd.read_csv(file_path))\n",
    "    \n",
    "    @property\n",
    "    def ifc_pywr_map(self) -> Dict[str, str]:\n",
    "        \"\"\" \"\"\"\n",
    "        return self.name_map.set_index('ifc_name')['pywr_name'].to_dict()\n",
    "    \n",
    "    @property\n",
    "    def pywr_ifc_map(self) -> Dict[str, str]:\n",
    "        \"\"\" \"\"\"\n",
    "        return self.name_map.set_index('pywr_name')['ifc_name'].to_dict()\n",
    "        \n",
    "    def print_cls(self) -> None:\n",
    "        print(self.name_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c266cf94",
   "metadata": {},
   "outputs": [],
   "source": [
    "col_name_map_reemission_inputs = {\n",
    "    \"res_area_fractions_2\": \"urban_area_fraction_1\",\n",
    "    \"res_area_fractions_5\": \"crop_area_fraction_1\",\n",
    "    \"res_area_fractions_7\": \"forest_area_fraction_1\",\n",
    "    \"res_area_fractions_11\": \"urban_area_fraction_2\",\n",
    "    \"res_area_fractions_14\": \"crop_area_fraction_2\",\n",
    "    \"res_area_fractions_16\": \"forest_area_fraction_2\",\n",
    "    \"res_area_fractions_20\": \"urban_area_fraction_3\",\n",
    "    \"res_area_fractions_23\": \"crop_area_fraction_3\",\n",
    "    \"res_area_fractions_25\": \"forest_area_fraction_3\",\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9985030",
   "metadata": {},
   "source": [
    "## 1. Import the required datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ac590ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import tabular heet output data, pywr output hp summary data and shp data (for spatial inference)\n",
    "reservoirs_shp_data_path = pathlib.Path(\"bin/heet_outputs_MIN_LOW_PRI/reservoirs_updated.shp\")\n",
    "hp_summary_data_path = pathlib.Path(\"intermediate/merged_table.xlsx\")\n",
    "reemission_output_data_path = pathlib.Path(\"outputs/reemission/combined/combined_outputs.csv\")\n",
    "ifc_pywr_map_path = pathlib.Path(\"config/ifc_pywr_name_map.csv\")\n",
    "ifc_db_shp_path = pathlib.Path(\"bin/gis_layers/ifc_database/all_dams_replaced_refactored.shp\")\n",
    "\n",
    "pywr_ifc_mapper = PywrIFCMapper.from_file(ifc_pywr_map_path)\n",
    "\n",
    "# 0. IFC database of dams\n",
    "ifc_gdf = gpd.read_file(ifc_db_shp_path)\\\n",
    "    .loc[:,['IFC_ID', 'geometry']]\\\n",
    "    .rename(columns={'IFC_ID': 'ifc_id'})\n",
    "\n",
    "# 1. Import spatial data and remove all reservoirs that are not HP or multipurpose\n",
    "reservoirs_gdf = gpd.read_file(reservoirs_shp_data_path)\\\n",
    "    .query(\"type == 'hydroelectric' | type == 'multipurpose'\")\\\n",
    "    .loc[:,['id', 'r_area_km2', 'geometry']]\\\n",
    "    .rename(columns={\"id\": \"ifc_id\"})\n",
    "\n",
    "reservoirs_gdf_all = gpd.read_file(reservoirs_shp_data_path)\\\n",
    "    .query(\"type == 'hydroelectric' | type == 'multipurpose'\")\\\n",
    "    .rename(columns={\"id\": \"ifc_id\"})\n",
    "\n",
    "# 2. Import tabular data with emissions and hp production\n",
    "hp_summary = pd.read_excel(hp_summary_data_path)\\\n",
    "    .loc[:,[\n",
    "        'ifc_id', 'dam_name', 'res_area', 'status_2_ifc', 'ro_r_or_sto_ifc', 'mean', 'pctile_10', \n",
    "        'co2_total_per_year', 'ch4_total_per_year', 'hp_type_reem']]\n",
    "hp_summary['status'] = np.where(hp_summary['status_2_ifc'].isin(['E']), 'Existing', 'Future')\n",
    "hp_summary = hp_summary\\\n",
    "    .assign(**{col: hp_summary[col].fillna(0) for col in \n",
    "               ['co2_total_per_year', 'ch4_total_per_year', 'res_area']})\\\n",
    "    .drop_duplicates()\\\n",
    "    .assign(dam_name=hp_summary['dam_name'].replace(pywr_ifc_mapper.pywr_ifc_map))\\\n",
    "    .sort_values(by=\"dam_name\", ascending=True)\\\n",
    "    .eval('tot_em = co2_total_per_year + ch4_total_per_year')\\\n",
    "    .drop(columns=['co2_total_per_year', 'ch4_total_per_year', 'status_2_ifc', 'res_area'])\\\n",
    "    .rename(columns={\n",
    "        \"mean\": \"HP_mean\", \"pctile_10\": \"HP_firm\", 'ro_r_or_sto_ifc' : 'hp_type_ifc',\n",
    "        \"dam_name\": \"name\"})\n",
    "\n",
    "# 3. Import the inputs tab from the outputs file from re-emission\n",
    "reemission_inputs = pd.read_csv(reemission_output_data_path)\\\n",
    "    .loc[:,[\n",
    "        'id', 'type', 'Scenario', 'res_area_fractions_2', \"res_area_fractions_5\",\n",
    "        \"res_area_fractions_7\", \"res_area_fractions_11\", \"res_area_fractions_14\", \"res_area_fractions_16\",\n",
    "        \"res_area_fractions_20\", \"res_area_fractions_23\", \"res_area_fractions_25\"]]\\\n",
    "    .query(\"type != 'irrigation' & Scenario == 'MIN_LOW_PRIM'\")\\\n",
    "    .rename(columns=col_name_map_reemission_inputs)\\\n",
    "    .rename(columns={'id': 'ifc_id'})\\\n",
    "    .eval('urban_area_fraction = urban_area_fraction_1 + urban_area_fraction_2 + urban_area_fraction_3')\\\n",
    "    .eval('forest_area_fraction = forest_area_fraction_1 + forest_area_fraction_2 + forest_area_fraction_3')\\\n",
    "    .eval('crop_area_fraction = crop_area_fraction_1 + crop_area_fraction_2 + crop_area_fraction_3')\\\n",
    "    .drop(columns=[\n",
    "        \"Scenario\", \"urban_area_fraction_1\", \"urban_area_fraction_2\", \"urban_area_fraction_3\",\n",
    "        \"crop_area_fraction_1\", \"crop_area_fraction_2\", \"crop_area_fraction_3\",\n",
    "        \"forest_area_fraction_1\", \"forest_area_fraction_2\", \"forest_area_fraction_3\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b34ebda6",
   "metadata": {},
   "outputs": [],
   "source": [
    "reservoirs_gdf_all.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64b48ed6",
   "metadata": {},
   "source": [
    "## 2. Join all three dataframes together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9142a82",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = reemission_inputs\\\n",
    "    .merge(hp_summary, on='ifc_id', how='right')\\\n",
    "    .merge(reservoirs_gdf, on='ifc_id', how='left')\n",
    "\n",
    "merged_df = merged_df\\\n",
    "    .assign(**{col: merged_df[col].fillna(0) for col in \n",
    "               ['urban_area_fraction', 'forest_area_fraction', 'crop_area_fraction',\n",
    "                'r_area_km2']})\\\n",
    "    .merge(ifc_gdf, on='ifc_id', suffixes=('_df1', '_ifc'))\n",
    "\n",
    "\n",
    "merged_df = merged_df\\\n",
    "    .assign(**{col: merged_df[col].fillna(merged_df['geometry_ifc']) for col in \n",
    "               ['geometry_df1']})\\\n",
    "    .assign(**{col: merged_df[col].fillna('hydroelectric') for col in \n",
    "               ['type']})\\\n",
    "    .drop(columns='geometry_ifc')\\\n",
    "    .rename(columns={'geometry_df1': 'geometry'})\\\n",
    "    .eval('urban_area_loss_km2 = urban_area_fraction * r_area_km2')\\\n",
    "    .eval('forest_area_loss_km2 = forest_area_fraction * r_area_km2')\\\n",
    "    .eval('crop_area_loss_km2 = crop_area_fraction * r_area_km2')\\\n",
    "    .drop(columns=['urban_area_fraction', 'forest_area_fraction', 'crop_area_fraction', 'r_area_km2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eff7e10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join dataframes for plotting maps (visualising MOO results)\n",
    "merged_df_plot = reemission_inputs\\\n",
    "    .merge(hp_summary, on='ifc_id', how='right')\\\n",
    "    .merge(reservoirs_gdf_all, on='ifc_id', how='left')\n",
    "_\n",
    "merged_df_plot = merged_df_plot\\\n",
    "    .assign(**{col: merged_df_plot[col].fillna(0) for col in \n",
    "               ['urban_area_fraction', 'forest_area_fraction', 'crop_area_fraction',\n",
    "                'r_area_km2']})\\\n",
    "    .merge(ifc_gdf, on='ifc_id', suffixes=('_df1', '_ifc'))\\\n",
    "    .drop(columns = [\"type_x\", \"urban_area_fraction\",\"c_biome\", \"c_soil_typ\", \"c_mean_slo\", \"n_populati\",\n",
    "                    \"r_msocs_kg\", \"r_mghr_all\", \"r_mghr_may\", \"r_mghr_nov\", \"r_mean_ann\", \"c_mar_mm\",\n",
    "                    \"c_map_mm\", \"c_mpet_mm\", \"c_masm_mm\", \"c_area_km2\", \"c_mean_ols\",\n",
    "                    \"ms_length\", \"tot_em_y\", \"geometry_df1\"])\n",
    "# Calculate composite values for plotting\n",
    "merged_df_plot[\"coordinates_1\"] = merged_df_plot.geometry_ifc.apply(lambda p: p.x)\n",
    "merged_df_plot[\"coordinates_0\"] = merged_df_plot.geometry_ifc.apply(lambda p: p.y)\n",
    "merged_df_plot['HP Production [GWh/year]'] = merged_df_plot[\"HP_mean\"] * 365.25 * 24 / 1_000\n",
    "merged_df_plot['Mean HP [GWh/d]'] = merged_df_plot[\"HP_mean\"] * 24 / 1_000\n",
    "merged_df_plot['Firm HP [GWh/d]'] = merged_df_plot['HP_firm'] * 24 / 1_000\n",
    "merged_df_plot['Firm Power Ratio, [%]'] = merged_df_plot['Firm HP [GWh/d]'] / merged_df_plot['Mean HP [GWh/d]'] * 100\n",
    "# Fill tot_em values for RoR HP\n",
    "# Force emission intensity of 3 gCO2eq/kWh\n",
    "ror_em_intensity = 3\n",
    "merged_df_plot.loc[merged_df_plot['tot_em_x'] == 0, 'tot_em_x'] = \\\n",
    "    merged_df_plot['HP_mean'] * ror_em_intensity / 1_000 * 365.25 * 24\n",
    "merged_df_plot['GHG intensity [gCO2,eq/kWh]'] = \\\n",
    "    merged_df_plot['tot_em_x'] / merged_df_plot['HP_mean'] * 1_000 / 365.25 / 24\n",
    "merged_df_plot['Volume, Mm3'] = merged_df_plot['r_volume_m'] / 1_000_000\n",
    "merged_df_plot['Area, km2'] = merged_df_plot['r_area_km2']\n",
    "# Note: tot_em is in tCO2eq yr-1\n",
    "merged_df_plot['GHG, tCO2eq/yr'] = merged_df_plot['tot_em_x']\n",
    "merged_df_plot = merged_df_plot\\\n",
    "    .drop(columns=['tot_em_x', 'r_volume_m', 'r_area_km2'])\n",
    "# Fill NA volume values with default_ror_volume\n",
    "default_ror_volume = 5 # Mm3\n",
    "merged_df_plot['Volume, Mm3'] = merged_df_plot['Volume, Mm3'].fillna(default_ror_volume)\n",
    "merged_df_plot.to_csv(pathlib.Path(\"intermediate/dams_for_plotting_moo.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5be2d16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the desired column order\n",
    "col_order = [\n",
    "    'ifc_id', 'name', 'type', 'status', 'hp_type_ifc', 'hp_type_reem',\n",
    "    'HP_mean', 'HP_firm', 'tot_em', 'urban_area_loss_km2', 'forest_area_loss_km2',\n",
    "    'crop_area_loss_km2', 'geometry']\n",
    "\n",
    "assert set(merged_df.columns) == set(col_order)\n",
    "\n",
    "# Reorder columns based on the desired order\n",
    "merged_df = merged_df[col_order]\n",
    "merged_gdf = gpd.GeoDataFrame(merged_df, geometry=merged_df['geometry'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95d9476b",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_gdf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18635204",
   "metadata": {},
   "source": [
    "## 3. Intersect the merged dataframe with village data points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eefff33e",
   "metadata": {},
   "outputs": [],
   "source": [
    "villages_gdf = gpd.read_file(\n",
    "    pathlib.Path(\n",
    "        \"bin/gis_layers/hotosm_mmr_populated_places_points_shp/hotosm_mmr_populated_places_points.shp\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b8395be",
   "metadata": {},
   "outputs": [],
   "source": [
    "flooded_villages = \\\n",
    "    gpd.sjoin(villages_gdf, merged_gdf, how='left', op='within')\\\n",
    "    .groupby('name_right').count()['osm_id'].reset_index()\\\n",
    "    .rename(columns={\"osm_id\" : \"count\", \"name_right\" : \"name\"})\n",
    "\n",
    "flooded_villages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52329910",
   "metadata": {},
   "source": [
    "## 4. Add information about flooded villages to `merged_gdf`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d9444a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_gdf = merged_gdf.merge(flooded_villages, on=\"name\", how='left')\\\n",
    "    .rename(columns={\"count\" : \"flooded_villages\"})\n",
    "final_gdf[\"flooded_villages\"] = final_gdf[\"flooded_villages\"].fillna(0)\n",
    "final_gdf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f1d180e",
   "metadata": {},
   "source": [
    "## 5. Save dataframes for post-processing\n",
    "1. Save Future reservoirs for optimization purposes\n",
    "2. Save Existing reservoirs for statistics on existing hydroelectric reservoirs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e05d341e",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_folder = pathlib.Path(\"outputs/moo\")\n",
    "if not output_folder.exists():\n",
    "    output_folder.mkdir()\n",
    "# Add status_int column\n",
    "final_gdf['status_int'] = final_gdf['status'].map({'Existing': 1, 'Future': 0})\n",
    "final_gdf\\\n",
    "    .drop(columns=['geometry'])\\\n",
    "    .to_csv(output_folder/'all_hp.csv')\n",
    "    \n",
    "final_gdf\\\n",
    "    .query('status == \"Existing\"')\\\n",
    "    .drop(columns=['geometry'])\\\n",
    "    .to_csv(output_folder/'existing_hp.csv')\n",
    "\n",
    "final_gdf\\\n",
    "    .query('status == \"Future\"')\\\n",
    "    .drop(columns=['geometry', 'type', 'status', 'hp_type_ifc', 'hp_type_reem'])\\\n",
    "    .to_csv(output_folder/'future_hp.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eed08c4e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
